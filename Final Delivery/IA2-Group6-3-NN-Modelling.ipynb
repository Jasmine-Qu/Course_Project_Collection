{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>...</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://mashable.com/2013/01/07/amazon-instant-...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>593</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://mashable.com/2013/01/07/ap-samsung-spon...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.604743</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.791946</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.118750</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>711</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://mashable.com/2013/01/07/apple-40-billio...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0.575130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.466667</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://mashable.com/2013/01/07/astronaut-notre...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.665635</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.369697</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://mashable.com/2013/01/07/att-u-verse-apps/</td>\n",
       "      <td>731.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>0.415646</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.540890</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.220192</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>505</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  timedelta  \\\n",
       "0  http://mashable.com/2013/01/07/amazon-instant-...      731.0   \n",
       "1  http://mashable.com/2013/01/07/ap-samsung-spon...      731.0   \n",
       "2  http://mashable.com/2013/01/07/apple-40-billio...      731.0   \n",
       "3  http://mashable.com/2013/01/07/astronaut-notre...      731.0   \n",
       "4   http://mashable.com/2013/01/07/att-u-verse-apps/      731.0   \n",
       "\n",
       "   n_tokens_title  n_tokens_content  n_unique_tokens  n_non_stop_words  \\\n",
       "0            12.0             219.0         0.663594               1.0   \n",
       "1             9.0             255.0         0.604743               1.0   \n",
       "2             9.0             211.0         0.575130               1.0   \n",
       "3             9.0             531.0         0.503788               1.0   \n",
       "4            13.0            1072.0         0.415646               1.0   \n",
       "\n",
       "   n_non_stop_unique_tokens  num_hrefs  num_self_hrefs  num_imgs  ...  \\\n",
       "0                  0.815385        4.0             2.0       1.0  ...   \n",
       "1                  0.791946        3.0             1.0       1.0  ...   \n",
       "2                  0.663866        3.0             1.0       1.0  ...   \n",
       "3                  0.665635        9.0             0.0       1.0  ...   \n",
       "4                  0.540890       19.0            19.0      20.0  ...   \n",
       "\n",
       "   max_positive_polarity  avg_negative_polarity  min_negative_polarity  \\\n",
       "0                    0.7              -0.350000                 -0.600   \n",
       "1                    0.7              -0.118750                 -0.125   \n",
       "2                    1.0              -0.466667                 -0.800   \n",
       "3                    0.8              -0.369697                 -0.600   \n",
       "4                    1.0              -0.220192                 -0.500   \n",
       "\n",
       "   max_negative_polarity  title_subjectivity  title_sentiment_polarity  \\\n",
       "0              -0.200000            0.500000                 -0.187500   \n",
       "1              -0.100000            0.000000                  0.000000   \n",
       "2              -0.133333            0.000000                  0.000000   \n",
       "3              -0.166667            0.000000                  0.000000   \n",
       "4              -0.050000            0.454545                  0.136364   \n",
       "\n",
       "   abs_title_subjectivity  abs_title_sentiment_polarity  shares  category  \n",
       "0                0.000000                      0.187500     593         0  \n",
       "1                0.500000                      0.000000     711         0  \n",
       "2                0.500000                      0.000000    1500         1  \n",
       "3                0.500000                      0.000000    1200         0  \n",
       "4                0.045455                      0.136364     505         0  \n",
       "\n",
       "[5 rows x 62 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('OnlineNewsPopularity.csv')\n",
    "data.columns=data.columns.str.replace(' ','')\n",
    "bins = [0, 1400, 10000, np.inf]\n",
    "data['category'] = pd.cut(data['shares'], bins, \n",
    "                          labels = ['Unpopular', 'Popular', 'Extremely_popular'])\n",
    "my_dict={'Unpopular':0,'Popular':1,'Extremely_popular':2}\n",
    "#inv_dict={0:'Unpopular',1:'Popular',2:'Extremely_popular'}\n",
    "data['category']=data['category'].map(my_dict)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_selected=['n_unique_tokens',\n",
    " 'kw_max_avg',\n",
    " 'kw_avg_avg',\n",
    " 'self_reference_avg_sharess',\n",
    " 'n_non_stop_unique_tokens',\n",
    " 'kw_avg_min',\n",
    " 'self_reference_min_shares',\n",
    " 'LDA_00',\n",
    " 'LDA_02',\n",
    " 'data_channel_is_socmed',\n",
    " 'data_channel_is_tech',\n",
    " 'kw_min_min',\n",
    " 'kw_avg_max',\n",
    " 'kw_min_avg',\n",
    " 'average_token_length',\n",
    " 'is_weekend',\n",
    " 'data_channel_is_entertainment',\n",
    " 'n_non_stop_words',\n",
    " 'LDA_03',\n",
    " 'weekday_is_friday',\n",
    " 'kw_max_min',\n",
    " 'kw_max_max',\n",
    " 'num_imgs',\n",
    " 'LDA_04',\n",
    " 'weekday_is_saturday',\n",
    " 'LDA_01',\n",
    " 'global_subjectivity',\n",
    " 'weekday_is_sunday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_features = ['data_channel_is_entertainment', 'data_channel_is_bus',\n",
    "       'data_channel_is_socmed', 'data_channel_is_tech',\n",
    "       'data_channel_is_world','weekday_is_monday', 'weekday_is_tuesday',\n",
    "       'weekday_is_wednesday', 'weekday_is_thursday', 'weekday_is_friday',\n",
    "       'weekday_is_saturday', 'weekday_is_sunday', 'is_weekend']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = ['n_tokens_title', 'n_tokens_content',\n",
    "       'n_unique_tokens', 'n_non_stop_words', 'n_non_stop_unique_tokens',\n",
    "       'num_hrefs', 'num_self_hrefs', 'num_imgs', 'num_videos',\n",
    "       'average_token_length', 'num_keywords','kw_min_min', 'kw_max_min', 'kw_avg_min',\n",
    "       'kw_min_max', 'kw_max_max', 'kw_avg_max', 'kw_min_avg', 'kw_max_avg',\n",
    "       'kw_avg_avg', 'self_reference_min_shares', 'self_reference_max_shares',\n",
    "       'self_reference_avg_sharess','LDA_00',\n",
    "       'LDA_01', 'LDA_02', 'LDA_03', 'LDA_04', 'global_subjectivity',\n",
    "       'global_sentiment_polarity', 'global_rate_positive_words',\n",
    "       'global_rate_negative_words', 'rate_positive_words',\n",
    "       'rate_negative_words', 'avg_positive_polarity', 'min_positive_polarity',\n",
    "       'max_positive_polarity', 'avg_negative_polarity',\n",
    "       'min_negative_polarity', 'max_negative_polarity', 'title_subjectivity',\n",
    "       'title_sentiment_polarity', 'abs_title_subjectivity',\n",
    "       'abs_title_sentiment_polarity']\n",
    "numerical_features_selected = [i for i in numerical_features if i in features_selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model build\n",
    "def Nerual_Network_model(layer_af='tanh',num_layers=1,optimizer='adam'):\n",
    "    #sgd、adam、RMSprop\n",
    "    model=Sequential()\n",
    "    model.add(Dense(256,activation=layer_af))\n",
    "    for i in range(num_layers):\n",
    "        model.add(Dense(512,activation=layer_af))\n",
    "    model.add(Dense(256,activation=layer_af))\n",
    "    model.add(Dense(64,activation=layer_af))\n",
    "    model.add(Dense(3,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "class balance_data():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def under_sampling(X,y):\n",
    "        rus = RandomUnderSampler()\n",
    "        X_RUS, y_RUS = rus.fit_sample(X,y)\n",
    "        return X_RUS, y_RUS\n",
    "    \n",
    "    def over_sampling(X,y):\n",
    "        #X_text,y_label are pd.Series\n",
    "        ros = RandomOverSampler()\n",
    "        X_ROS, y_ROS = ros.fit_sample(X,y)\n",
    "        return X_ROS, y_ROS\n",
    "\n",
    "    def no_sampling(X,y):\n",
    "        return X,y\n",
    "    \n",
    "    def smote_sampling(X,y):\n",
    "        sm = SMOTE()\n",
    "        X_SMOTE, y_SMOTE = sm.fit_sample(X,y)\n",
    "        return X_SMOTE, y_SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.80      0.68      4017\n",
      "           1       0.60      0.42      0.50      3475\n",
      "           2       0.22      0.00      0.01       437\n",
      "\n",
      "    accuracy                           0.59      7929\n",
      "   macro avg       0.47      0.41      0.40      7929\n",
      "weighted avg       0.57      0.59      0.56      7929\n",
      "\n",
      "----------------------------\n",
      "Accuracy: 0.593391\n",
      "----------------------------\n",
      "F1-score: 0.39567102585768715\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[category_features+numerical_features],data['category'],\n",
    "                                                    stratify=data['category'],test_size=0.2,random_state=2021)\n",
    "X_train_norm = X_train.copy()\n",
    "X_test_norm = X_test.copy()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X_train[numerical_features])\n",
    "X_train_norm[numerical_features] = scaler.transform(X_train[numerical_features])\n",
    "X_test_norm[numerical_features] = scaler.transform(X_test_norm[numerical_features])\n",
    "encoder = OneHotEncoder()\n",
    "y_train_dummy = encoder.fit_transform(np.asarray(y_train).reshape(-1,1))\n",
    "\n",
    "X_train_norm=np.asarray(X_train_norm)\n",
    "X_test_norm=np.asarray(X_test_norm)\n",
    "y_train_dummy=y_train_dummy.toarray()\n",
    "\n",
    "X_train_norm_b,y_train_dummy_b=balance_data.no_sampling(X_train_norm,y_train_dummy)\n",
    "\n",
    "model=Nerual_Network_model()\n",
    "model.fit(x=X_train_norm_b,y=y_train_dummy_b,verbose=0)\n",
    "y_pred=np.argmax(model.predict(X_test_norm),axis=1)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test,y_pred))\n",
    "print('----------------------------')\n",
    "print(\"Accuracy: %f\" %(accuracy_score(y_test, y_pred)))\n",
    "print('----------------------------')\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## features-selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.63      0.64      4017\n",
      "           1       0.56      0.64      0.60      3475\n",
      "           2       0.00      0.00      0.00       437\n",
      "\n",
      "    accuracy                           0.60      7929\n",
      "   macro avg       0.40      0.42      0.41      7929\n",
      "weighted avg       0.57      0.60      0.58      7929\n",
      "\n",
      "----------------------------\n",
      "Accuracy: 0.600202\n",
      "----------------------------\n",
      "F1-score: 0.41112424546874476\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[features_selected],data['category'],\n",
    "                                                    stratify=data['category'],test_size=0.2,random_state=2021)\n",
    "X_train_norm = X_train.copy()\n",
    "X_test_norm = X_test.copy()\n",
    "scaler = StandardScaler()\n",
    "#scaler=MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(X_train[numerical_features_selected])\n",
    "X_train_norm[numerical_features_selected] = scaler.transform(X_train[numerical_features_selected])\n",
    "X_test_norm[numerical_features_selected] = scaler.transform(X_test_norm[numerical_features_selected])\n",
    "encoder = OneHotEncoder()\n",
    "y_train_dummy = encoder.fit_transform(np.asarray(y_train).reshape(-1,1))\n",
    "\n",
    "X_train_norm=np.asarray(X_train_norm)\n",
    "X_test_norm=np.asarray(X_test_norm)\n",
    "y_train_dummy=y_train_dummy.toarray()\n",
    "\n",
    "X_train_norm_b,y_train_dummy_b=balance_data.no_sampling(X_train_norm,y_train_dummy)\n",
    "\n",
    "model=Nerual_Network_model()\n",
    "model.fit(x=X_train_norm_b,y=y_train_dummy_b,verbose=0)\n",
    "y_pred=np.argmax(model.predict(X_test_norm),axis=1)\n",
    "\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test,y_pred))\n",
    "print('----------------------------')\n",
    "print(\"Accuracy: %f\" %(accuracy_score(y_test, y_pred)))\n",
    "print('----------------------------')\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## features-selection+random undersample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.59      0.61      4017\n",
      "           1       0.58      0.39      0.47      3475\n",
      "           2       0.11      0.49      0.18       437\n",
      "\n",
      "    accuracy                           0.50      7929\n",
      "   macro avg       0.44      0.49      0.42      7929\n",
      "weighted avg       0.59      0.50      0.52      7929\n",
      "\n",
      "----------------------------\n",
      "Accuracy: 0.495018\n",
      "----------------------------\n",
      "F1-score: 0.420192794007661\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[features_selected],data['category'],\n",
    "                                                    stratify=data['category'],test_size=0.2,random_state=2021)\n",
    "X_train_norm = X_train.copy()\n",
    "X_test_norm = X_test.copy()\n",
    "scaler = StandardScaler()\n",
    "#scaler=MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(X_train[numerical_features_selected])\n",
    "X_train_norm[numerical_features_selected] = scaler.transform(X_train[numerical_features_selected])\n",
    "X_test_norm[numerical_features_selected] = scaler.transform(X_test_norm[numerical_features_selected])\n",
    "encoder = OneHotEncoder()\n",
    "y_train_dummy = encoder.fit_transform(np.asarray(y_train).reshape(-1,1))\n",
    "\n",
    "X_train_norm=np.asarray(X_train_norm)\n",
    "X_test_norm=np.asarray(X_test_norm)\n",
    "y_train_dummy=y_train_dummy.toarray()\n",
    "\n",
    "X_train_norm_b,y_train_dummy_b=balance_data.under_sampling(X_train_norm,y_train_dummy)\n",
    "\n",
    "model=Nerual_Network_model()\n",
    "model.fit(x=X_train_norm_b,y=y_train_dummy_b,verbose=0)\n",
    "y_pred=np.argmax(model.predict(X_test_norm),axis=1)\n",
    "\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test,y_pred))\n",
    "print('----------------------------')\n",
    "print(\"Accuracy: %f\" %(accuracy_score(y_test, y_pred)))\n",
    "print('----------------------------')\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## features-selection+smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.53      0.59      4017\n",
      "           1       0.55      0.39      0.46      3475\n",
      "           2       0.11      0.57      0.18       437\n",
      "\n",
      "    accuracy                           0.47      7929\n",
      "   macro avg       0.45      0.50      0.41      7929\n",
      "weighted avg       0.59      0.47      0.51      7929\n",
      "\n",
      "----------------------------\n",
      "Accuracy: 0.471056\n",
      "----------------------------\n",
      "F1-score: 0.4110174805974489\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[features_selected],data['category'],\n",
    "                                                    stratify=data['category'],test_size=0.2,random_state=2021)\n",
    "X_train_norm = X_train.copy()\n",
    "X_test_norm = X_test.copy()\n",
    "scaler = StandardScaler()\n",
    "#scaler=MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(X_train[numerical_features_selected])\n",
    "X_train_norm[numerical_features_selected] = scaler.transform(X_train[numerical_features_selected])\n",
    "X_test_norm[numerical_features_selected] = scaler.transform(X_test_norm[numerical_features_selected])\n",
    "encoder = OneHotEncoder()\n",
    "y_train_dummy = encoder.fit_transform(np.asarray(y_train).reshape(-1,1))\n",
    "\n",
    "\n",
    "X_train_norm=np.asarray(X_train_norm)\n",
    "X_test_norm=np.asarray(X_test_norm)\n",
    "y_train_dummy=y_train_dummy.toarray()\n",
    "\n",
    "X_train_norm_b,y_train_dummy_b=balance_data.smote_sampling(X_train_norm,y_train_dummy)\n",
    "\n",
    "model=Nerual_Network_model()\n",
    "model.fit(x=X_train_norm_b,y=y_train_dummy_b,verbose=0)\n",
    "y_pred=np.argmax(model.predict(X_test_norm),axis=1)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test,y_pred))\n",
    "print('----------------------------')\n",
    "print(\"Accuracy: %f\" %(accuracy_score(y_test, y_pred)))\n",
    "print('----------------------------')\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## features-selection+ tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.73      0.68      4017\n",
      "           1       0.60      0.57      0.59      3475\n",
      "           2       0.22      0.00      0.01       437\n",
      "\n",
      "    accuracy                           0.62      7929\n",
      "   macro avg       0.49      0.44      0.43      7929\n",
      "weighted avg       0.60      0.62      0.60      7929\n",
      "\n",
      "----------------------------\n",
      "Accuracy: 0.621390\n",
      "----------------------------\n",
      "F1-score: 0.4254035645100944\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[features_selected],data['category'],\n",
    "                                                    stratify=data['category'],test_size=0.2,random_state=2021)\n",
    "X_train_norm = X_train.copy()\n",
    "X_test_norm = X_test.copy()\n",
    "scaler = StandardScaler()\n",
    "#scaler=MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(X_train[numerical_features_selected])\n",
    "X_train_norm[numerical_features_selected] = scaler.transform(X_train[numerical_features_selected])\n",
    "X_test_norm[numerical_features_selected] = scaler.transform(X_test_norm[numerical_features_selected])\n",
    "encoder = OneHotEncoder()\n",
    "y_train_dummy = encoder.fit_transform(np.asarray(y_train).reshape(-1,1))\n",
    "\n",
    "X_train_norm=np.asarray(X_train_norm)\n",
    "X_test_norm=np.asarray(X_test_norm)\n",
    "y_train_dummy=y_train_dummy.toarray()\n",
    "\n",
    "X_train_norm_b,y_train_dummy_b=balance_data.no_sampling(X_train_norm,y_train_dummy)\n",
    "\n",
    "model=Nerual_Network_model()\n",
    "model.fit(x=X_train_norm_b,y=y_train_dummy_b,batch_size=15000,epochs=50,verbose=0)\n",
    "y_pred=np.argmax(model.predict(X_test_norm),axis=1)\n",
    "\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test,y_pred))\n",
    "print('----------------------------')\n",
    "print(\"Accuracy: %f\" %(accuracy_score(y_test, y_pred)))\n",
    "print('----------------------------')\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
